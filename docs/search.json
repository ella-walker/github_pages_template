[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hello! My name is Ella Walker, and I am a student at Brigham Young University, studying Statistics. My passion for data science stems from a desire to uncover insights from data and apply them to real-world problems. I aspire to build a career in data science, focusing on community needs. This year, I had the opportunity to participate in a Gadsden Research Study with Alegra Learning, which further fueled my enthusiasm for data-driven decision-making.\nIn my spare time I like to read, knit, travel, and spend time with friends and family. Recently I knit my first hat! This summer I traveled to Yosemite National Park and had a great time hiking through the beautiful scenery."
  },
  {
    "objectID": "about.html#get-to-know-me",
    "href": "about.html#get-to-know-me",
    "title": "About Me",
    "section": "",
    "text": "Hello! My name is Ella Walker, and I am a student at Brigham Young University, studying Statistics. My passion for data science stems from a desire to uncover insights from data and apply them to real-world problems. I aspire to build a career in data science, focusing on community needs. This year, I had the opportunity to participate in a Gadsden Research Study with Alegra Learning, which further fueled my enthusiasm for data-driven decision-making.\nIn my spare time I like to read, knit, travel, and spend time with friends and family. Recently I knit my first hat! This summer I traveled to Yosemite National Park and had a great time hiking through the beautiful scenery."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\nBS Statistics - Brigham Young University, April, 2026 (Expected)\nMinor - Sociology\nRelevant Coursework: Statistics, Data Analysis, Programming, etc."
  },
  {
    "objectID": "about.html#skills-interests",
    "href": "about.html#skills-interests",
    "title": "About Me",
    "section": "Skills & Interests",
    "text": "Skills & Interests\n\nTechnical Skills\n\nProgramming: Python, R\nData Analysis: Pandas, NumPy\nVisualization: Matplotlib, Tidymodels\nTools: Jupyter Notebooks, Git/GitHub\n\n\n\nAreas of Interest\n\nSociology and community development\nEducation and learning technologies"
  },
  {
    "objectID": "about.html#experience-goals",
    "href": "about.html#experience-goals",
    "title": "About Me",
    "section": "Experience & Goals",
    "text": "Experience & Goals\n\nExperience\n\nInternship - Data Analyst Intern at Alegra Learning (Current)\n\nAnalyzing educational data to improve learning outcomes for EL students\nCreating visualizations to communicate findings\nReviewing and testing software features\n\nGadsden Research Study - As part of my internship at Alegra Learning I conducted research analysis on educational tools and their impact on learning outcome for their recent Gadsden Research Study\nData Science Projects - Various personal and academic projects involving data collection, cleaning, analysis, and visualization as I have worked through courses at BYU.\n\nThrough my data science journey, I aim to achieve the following goals:\n\nBuild a strong foundation in data science principles and techniques\nGain hands-on experience through projects and internships\nContribute to meaningful projects that address community challenges\nDevelop a professional network in the data science field"
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About Me",
    "section": "Contact",
    "text": "Contact\n\nEmail: wella2@byu.edu\nGitHub: github.com/ella-walker\nLinkedIn: linkedin.com/in/ella-walker3\n\n\n\n\n\nElla Walker\n\n\nThis portfolio showcases my learning progress and projects completed during my data science studies."
  },
  {
    "objectID": "projects/blog.html",
    "href": "projects/blog.html",
    "title": "Cleaning Messy Columns Blog",
    "section": "",
    "text": "Messy or inconsistent columns can make your data difficult to understand and work with. If your dataset is poorly formatted, you may notice you are spending more of your project trying to understand your own data than analyzing it. Fortunately, Python offers a very simple way to change your column names through pandas to make your analysis easier and more reliable.\nIn this post we will compile some common column issues into an easy step by step list that you can repeat on any project.",
    "crumbs": [
      "Cleaning Messy Columns Blog"
    ]
  },
  {
    "objectID": "projects/blog.html#cleaning-messy-column-names-in-seconds-with-pandas",
    "href": "projects/blog.html#cleaning-messy-column-names-in-seconds-with-pandas",
    "title": "Cleaning Messy Columns Blog",
    "section": "",
    "text": "Messy or inconsistent columns can make your data difficult to understand and work with. If your dataset is poorly formatted, you may notice you are spending more of your project trying to understand your own data than analyzing it. Fortunately, Python offers a very simple way to change your column names through pandas to make your analysis easier and more reliable.\nIn this post we will compile some common column issues into an easy step by step list that you can repeat on any project.",
    "crumbs": [
      "Cleaning Messy Columns Blog"
    ]
  },
  {
    "objectID": "projects/blog.html#why-is-cleaning-data-important",
    "href": "projects/blog.html#why-is-cleaning-data-important",
    "title": "Cleaning Messy Columns Blog",
    "section": "Why is cleaning data important?",
    "text": "Why is cleaning data important?\nData cleaning is one of the essential steps in the data science lifecycle, as pictured below. Before using a dataset you must clean it to ensure accuracy, consistency, and completeness. Dirty data can lead to the wrong conclusions, confusing bugs, and wasted time.\nColumn names may seem trivial but they are actually an important part of a clean data set because they not only make the data clear for the analyst, but also improve data integrity for all potential users.\n\n\n\nImage credit: Analytics Training Hub (https://analyticstraininghub.com/life-cycle-of-data-science/)\n\n\nBy investing a few minutes to clean your column names, you may be saving yourself hours in debugging and ensuring reliable results.",
    "crumbs": [
      "Cleaning Messy Columns Blog"
    ]
  },
  {
    "objectID": "projects/blog.html#how-do-i-use-pandas-to-make-consistent-column-names",
    "href": "projects/blog.html#how-do-i-use-pandas-to-make-consistent-column-names",
    "title": "Cleaning Messy Columns Blog",
    "section": "How do I use pandas to make consistent column names?",
    "text": "How do I use pandas to make consistent column names?\nCleaning column names in pandas can be simplified to three main steps:\nStep 1: Remove leading and trailing whitespace\nStep 2: Standardize formatting\nStep 3: Remove special characters\nOnce you understand these steps, you can combine them into a single repeatable function which will clean all your datasets in seconds.",
    "crumbs": [
      "Cleaning Messy Columns Blog"
    ]
  },
  {
    "objectID": "projects/blog.html#putting-it-all-together",
    "href": "projects/blog.html#putting-it-all-together",
    "title": "Cleaning Messy Columns Blog",
    "section": "Putting it all Together",
    "text": "Putting it all Together\nNow that you have learned these three steps, you can combine them into a single function to quickly and consistently clean any dataset.\n\nmovies = pd.DataFrame({\n    \" Title \": [\"Superman\", \"Fantastic Four\", \"Dog Man\"],\n    \" Box Office Amount in Millions $ \": [600, 519, 145]\n})\n\ndef clean_columns(data):\n    data.columns = (\n        data.columns\n        .str.strip()\n        .str.lower()\n        .str.replace(\" \", \"_\")\n        .str.replace(r'[^a-zA-Z0-9_]', '', regex=True)\n    )\n    return data\n\n# Running our new cleaning function\nmovies = clean_columns(movies)\n\nmovies\n\n\n\n\n\n\n\n\ntitle\nbox_office_amount_in_millions_\n\n\n\n\n0\nSuperman\n600\n\n\n1\nFantastic Four\n519\n\n\n2\nDog Man\n145",
    "crumbs": [
      "Cleaning Messy Columns Blog"
    ]
  },
  {
    "objectID": "projects/blog.html#conclusion",
    "href": "projects/blog.html#conclusion",
    "title": "Cleaning Messy Columns Blog",
    "section": "Conclusion",
    "text": "Conclusion\nMessy column names don’t have to slow you down! By removing whitespace, standardizing formatting, and removing special chaaracters you can quickly make your dataset more readable and consistent, and less prone to errors.\nWith pandas, this process is repeatable and scalable, so you can apply it to any dataset in seconds. Make sure to add this step to your data cleaning workflow, your future self (and others) will thank you for it!",
    "crumbs": [
      "Cleaning Messy Columns Blog"
    ]
  },
  {
    "objectID": "projects/final-project.html",
    "href": "projects/final-project.html",
    "title": "Final Project",
    "section": "",
    "text": "This is coming down the pipeline. Check again later.",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "projects/data-acquisition.html",
    "href": "projects/data-acquisition.html",
    "title": "Data Acquisition Project",
    "section": "",
    "text": "For this project I created my own dataset in which I used webscraping techniques to create a unique dataframe that I could explore. I chose to explore information on mass shootings in the United States so far in 2025. I was curious when and where shootings occurred most frequently, how many people died or were injured in mass shootings, and if there were any common characteristics between these tragic events.\nI believe that if we can understand this data then we can better create policy and regulation which will serve all American residents and promote peace and safety for everyone.\nI confirmed that the data I gathered from Wikipedia is publicly available and can be freely used for educational purposes. I made sure to include a user-agent in my requests and kept my scraping minimal to avoid overloading the site.",
    "crumbs": [
      "Data Acquisition Project"
    ]
  },
  {
    "objectID": "projects/data-acquisition.html#victims-by-month",
    "href": "projects/data-acquisition.html#victims-by-month",
    "title": "Data Acquisition Project",
    "section": "Victims by Month",
    "text": "Victims by Month\nThe plot below is a bar graph showing total victims by month with deaths shown in red and injuries shown in blue. Interestingly, the spike noted in July doesn’t correspond to a single major shooting but rather several smaller events scattered across the country. This highlights the widespread nature of the problem rather than isolated incidents.\nThis also potentially highlights a seasonal trend of shootings. Mass shootings seem to be higher across summer months than any other months of the year. In future reserach I am curious to see if this trend of summer shootings is common across time.\n\n\nCode\nimport sys\nprint(sys.executable)\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"../mass_shootings_2025.csv\")\n\n# Prepare data\ndf['Month'] = pd.to_datetime(df['Date']).dt.month\nmonthly = df.groupby('Month')[['Total Deaths', 'Total Injuries']].sum().reset_index()\n\n# Plot stacked bars\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot deaths\nax.bar(monthly['Month'], monthly['Total Deaths'], color='DarkRed', label='Deaths')\n\n# Plot injuries on top\nax.bar(monthly['Month'], monthly['Total Injuries'], \n       bottom=monthly['Total Deaths'], color='SteelBlue', label='Injuries')\n\n# Labels and title\nax.set_title('Total Deaths and Injuries by Month in 2025')\nax.set_xlabel('Month')\nax.set_ylabel('Count')\nax.set_xticks(range(1, 13))\nax.set_xticklabels(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'])\nax.legend()\n\n# Note below plot\nplt.figtext(0.55, -0.02,\n            \"Note: This graph was created on November 12, 2025.\\nShootings for November and December are incomplete.\")\n\nplt.tight_layout()\nplt.show()\n\n\n/opt/miniconda3/bin/python\n\n\n\n\n\n\n\n\nFigure 1: Total Victims by Month",
    "crumbs": [
      "Data Acquisition Project"
    ]
  },
  {
    "objectID": "projects/data-acquisition.html#distribution-of-victims-per-shooting",
    "href": "projects/data-acquisition.html#distribution-of-victims-per-shooting",
    "title": "Data Acquisition Project",
    "section": "Distribution of Victims per Shooting",
    "text": "Distribution of Victims per Shooting\nAfter looking at the distribution of victims by month of the year I was curious to know how many victims there are per shooting. The following boxplot highlights and median and interquartile range of the data. The data is clearly right skewed with the median falling around 5 and several outliers that range up to 30.\n\n\nCode\nimport seaborn as sns\nimport pandas as pd\n\n# Load your dataset\ndf = pd.read_csv(\"../mass_shootings_2025.csv\")\n\n# Compute total victims per shooting\ndf['Total Victims'] = df['Total Deaths'] + df['Total Injuries']\n\n\nplt.figure(figsize=(8,6))\nsns.boxplot(x=df['Total Victims'], color='lightcoral')\nplt.title(\"Boxplot of Victims per Shooting in 2025\")\nplt.xlabel(\"Number of Victims\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Boxplot of Distribution of Victims per Shooting",
    "crumbs": [
      "Data Acquisition Project"
    ]
  },
  {
    "objectID": "projects/data-acquisition.html#shootings-by-state",
    "href": "projects/data-acquisition.html#shootings-by-state",
    "title": "Data Acquisition Project",
    "section": "Shootings by State",
    "text": "Shootings by State\nI was also highly interested in if shootings were concentrated in a certain location. I used a bar chart to compare shootings by state.\nMost shootings occurred in Texas, California, and Illinois. Texas, California, and Illinois are all highly populated states, so containing a high proportion of shootings seems realistic. In future research I am curious to see if states with high shootings are strongly correlated with low income states or states with little gun regulation.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Example: your state counts\nstate_counts = df['State or Territory'].value_counts().reset_index()\nstate_counts.columns = ['State or Territory', 'Total Shootings']\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 8))\nbars = ax.barh(state_counts['State or Territory'], state_counts['Total Shootings'], color='teal')\n\n# Add numbers at the end of each bar\nfor bar in bars:\n    width = bar.get_width()\n    ax.text(width + 0.5, bar.get_y() + bar.get_height()/2,  # x-position, y-position\n            f'{int(width)}', va='center', ha='left', fontsize=10)\n\n# Labels and title\nax.set_xlabel('Total Shootings')\nax.set_title('Total Shootings by State or Territory')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Total Shootings by US State or Territory\n\n\n\n\n\nThis EDA answers my initial questions by showing that mass shootings are spread across the U.S., with certain months and states showing higher frequency. A next step could involve comparing 2025 to prior years or exploring correlations with socioeconomic or political events.",
    "crumbs": [
      "Data Acquisition Project"
    ]
  },
  {
    "objectID": "projects/eda.html",
    "href": "projects/eda.html",
    "title": "EDA Project",
    "section": "",
    "text": "This is coming down the pipeline. Check again later.",
    "crumbs": [
      "EDA Project"
    ]
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects Overview",
    "section": "",
    "text": "Description: Pick a dataset and explore it to discover insights and answer questions.\n\n\n\nDescription: Find an interesting data source, collect the data, and prepare it for analysis.\n\n\n\nDescription: A comprehensive project that shows off my data science skills.",
    "crumbs": [
      "Projects Overview"
    ]
  },
  {
    "objectID": "projects/index.html#all-projects",
    "href": "projects/index.html#all-projects",
    "title": "Projects Overview",
    "section": "",
    "text": "Description: Pick a dataset and explore it to discover insights and answer questions.\n\n\n\nDescription: Find an interesting data source, collect the data, and prepare it for analysis.\n\n\n\nDescription: A comprehensive project that shows off my data science skills.",
    "crumbs": [
      "Projects Overview"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "Welcome to my data science portfolio! This site shows my journey learning data science and analytics. Here you’ll find projects that demonstrate what I’ve learned and discovered.\n\n\nThis portfolio shows my work learning data science. Each project includes:\n\nMy code with documentation\nVisualizations I created\nWhat I learned and discovered\n\nI built this site using Quarto and host it on GitHub Pages.\n\n\n\n\nProgramming: Python, Pandas for data analysis\nVisualization: Creating charts with Matplotlib and Seaborn\nData Collection: Getting data from files, websites, and APIs\nAnalysis: Finding patterns and answering questions with data\n\n\n\n\n\n\n\nLearn how I explore datasets to find interesting patterns and answer questions.\n\n\n\nRead my thoughts on cleaning messy columns with pandas.\n\n\n\nSee how I gather data from different sources and prepare it for analysis.\n\n\n\nSee how I tackle a data science project beginning to end.\n\n\n\nThanks for visiting! Feel free to explore my projects and see what I’m learning."
  },
  {
    "objectID": "index.html#about-this-portfolio",
    "href": "index.html#about-this-portfolio",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "This portfolio shows my work learning data science. Each project includes:\n\nMy code with documentation\nVisualizations I created\nWhat I learned and discovered\n\nI built this site using Quarto and host it on GitHub Pages."
  },
  {
    "objectID": "index.html#skills-im-learning",
    "href": "index.html#skills-im-learning",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "Programming: Python, Pandas for data analysis\nVisualization: Creating charts with Matplotlib and Seaborn\nData Collection: Getting data from files, websites, and APIs\nAnalysis: Finding patterns and answering questions with data"
  },
  {
    "objectID": "index.html#my-projects",
    "href": "index.html#my-projects",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "Learn how I explore datasets to find interesting patterns and answer questions.\n\n\n\nRead my thoughts on cleaning messy columns with pandas.\n\n\n\nSee how I gather data from different sources and prepare it for analysis.\n\n\n\nSee how I tackle a data science project beginning to end.\n\n\n\nThanks for visiting! Feel free to explore my projects and see what I’m learning."
  }
]